{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_datasets_creation2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBkU30Vg0GSl"
      },
      "source": [
        "# function that takes 1 to 4 inputs and creates a different balanced datasets\n",
        "# DATA0.0 360 degree images from Google dataset\n",
        "# DATA0.1 360 degree images of our own\n",
        "\n",
        "# DATA0 360 degree images with fire extinguishers (this is what was in the validation folder)\n",
        "# DATA1 normal images from internet (oldouz labels )\n",
        "# DATA2 deformed images with data augmentation (ari labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVlQ_Mzh14c0"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jlo7Fh39mUy"
      },
      "source": [
        "import os\n",
        "# importing shutil module  \n",
        "import shutil\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiHOWMJ2fS3L"
      },
      "source": [
        "## google drive mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-luLmCFfUvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b067f8-d333-4f00-bbb0-071ce948b0b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_slMV3X66ps"
      },
      "source": [
        "# directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvzF2WhqIsYq"
      },
      "source": [
        "#change work directory\n",
        "os.chdir(\"/content/drive/My Drive/DATASET/\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xuTTb60IsYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7344c42-904a-4487-a31b-ab496fbf57f2"
      },
      "source": [
        "# change work directory and check on where you are \n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA_0\tDATA_1\tDATA_2\tdatasets_overview.xlsx\tdata.zip  test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ayrVmMAIsYt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "719ef49d-1059-4c0f-f858-3fe3651d37a3"
      },
      "source": [
        "# var my directory\n",
        "mydir = os.getcwd()\n",
        "mydir"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DATASET'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xP1Qv9JhhL"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Xtlkl1IBaiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8877e04d-0dee-4594-9cfc-d05997a52a38"
      },
      "source": [
        "# store subfolders\n",
        "DATASET = os.listdir(mydir)\n",
        "DATA = sorted(DATASET)[:3]\n",
        "DATA"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DATA_0', 'DATA_1', 'DATA_2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhZ_ZEwOl3Cj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77557b0a-4d61-456b-8c1d-0bfefa23f79f"
      },
      "source": [
        "DATASET = os.listdir(mydir)\n",
        "DATASET\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DATA_2', 'DATA_0', 'DATA_1', 'test', 'data.zip', 'datasets_overview.xlsx']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaaLHE-4Jmz1"
      },
      "source": [
        "## deformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IANO2ixmPXVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383d94c2-4580-46ff-c03c-2d1e98eb0f15"
      },
      "source": [
        "DEFORMATION = [\"High\",\"Low\", \"None\"]\n",
        "DEF = 1\n",
        "DEFORMATION[DEF:DEF+1]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Low']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NYBpVALJtkq"
      },
      "source": [
        "## check file type\n",
        "\n",
        ".png not admitted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJkOe8-5OZhm"
      },
      "source": [
        "#taking the extension out\n",
        "\n",
        "def find_all_extension_infolder(destination):\n",
        "  SplitTypes=[]\n",
        "  for file in os.listdir(destination):\n",
        "      SplitTypes.append(file.split('.')[-1])\n",
        "  set_extensions = list(set(SplitTypes))\n",
        "  return (set_extensions)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRBrp0CAOGTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52a017d-5117-4a73-e3da-cd79799d7fdb"
      },
      "source": [
        "# checking extensions out \n",
        "DATA_SIZE = []\n",
        "for sub in DATA:\n",
        "  set_extensions = find_all_extension_infolder(sub)\n",
        "  print (set_extensions)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['jpg', 'xml']\n",
            "['jpg', 'xml']\n",
            "['jpg', 'xml']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHMMavTTADYg"
      },
      "source": [
        "#delete_extensions = ['jpeg', 'png']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ftBPfbnJ0zh"
      },
      "source": [
        "## check folders\n",
        "\n",
        "take as images input only ,jpg files\n",
        "controls that for each images there is one annotation and vice versa\n",
        "discard the rest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7D-r9itBSyW"
      },
      "source": [
        "# count the number of elements in each subfolder\n",
        "# rememeber images and annotations are together so the number should be even \n",
        "def get_images_annotation_in_folders(DATA):\n",
        "  \"\"\"\n",
        "  DATA is a vector of sub folders\n",
        "  DATA_SIZE, data_dict,wrong_files = get_images_annotation_in_folders(DATA)\n",
        "  \"\"\"\n",
        "  DATA_SIZE = []\n",
        "  data_dict =  dict()\n",
        "  wrong_files = list()\n",
        "  for sub in DATA:\n",
        "    sub_dir =  (mydir + '/' +sub)\n",
        "    all_files = os.listdir(sub_dir)\n",
        "    DATA_SIZE.append(len(all_files))\n",
        "    annotations = [file for file in all_files if file.endswith(\".xml\")]\n",
        "    #images = [file for file in all_files if (file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".png\")) ] \n",
        "    images = [file for file in all_files if (file.endswith(\".jpg\"))]\n",
        "    others = []\n",
        "    #others = [file for file in others if not file in annotations ]\n",
        "    others = [file for file in all_files if (file.endswith(\".jpeg\") or file.endswith(\".png\"))]\n",
        "    wrong_files.extend(others)\n",
        "    data_dict[sub] = (images, annotations)\n",
        "    check = \"\"\n",
        "    if len(annotations) != len(images):\n",
        "      check = \"<------------ shite\"\n",
        "    print (\"%s contains %s files: %s images and %s annotations ,else %s %s\" %(sub_dir , len(all_files), len(images), len(annotations),len(others),check))\n",
        "    #if len(others) > 0:\n",
        "      #print (others)\n",
        "  return DATA_SIZE, data_dict, wrong_files"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T0yUUk7cMW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f21112e-5f3a-4aab-9cdc-5231174f721b"
      },
      "source": [
        "DATA_SIZE, data_dict,wrong_files = get_images_annotation_in_folders(DATA)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DATASET/DATA_0 contains 208 files: 104 images and 104 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_1 contains 800 files: 400 images and 400 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_2 contains 800 files: 400 images and 400 annotations ,else 0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZosrqR4KbtQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6990d4fd-0291-4810-9719-bb94e43313fe"
      },
      "source": [
        "DATA_SIZE"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[208, 800, 800]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px0-RnxzYDDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf8e1a8-1183-4c2f-eb42-b04bd8605f23"
      },
      "source": [
        "# remove unmatching images / annotations\n",
        "# refresh the data_dict!\n",
        "images_to_delete = []\n",
        "annotations_to_delete = []\n",
        "for k in data_dict.keys():\n",
        "  print (\"---%s----\" %k)\n",
        "  images, annotations = data_dict[k] # refresh the data_dict!\n",
        "  print (len(images), len(annotations))\n",
        "  set_a = set([ x[:-4] for x in images])\n",
        "  set_b = set([ x[:-4] for x in annotations])\n",
        "  images_to_delete = set_a-set_b\n",
        "  annotations_to_delete = set_b-set_a\n",
        "  print (len(images_to_delete),len(annotations_to_delete))\n",
        "\n",
        "  for im in images_to_delete:\n",
        "      fullpath = mydir+'/'+ k +'/'+ im + \".jpg\"\n",
        "      print (fullpath)\n",
        "      os.remove(fullpath)\n",
        "\n",
        "  for annotation in annotations_to_delete:\n",
        "      fullpath = mydir+'/'+ k +'/'+ annotation + \".xml\"\n",
        "      print (fullpath)\n",
        "      os.remove(fullpath)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---DATA_0----\n",
            "104 104\n",
            "0 0\n",
            "---DATA_1----\n",
            "400 400\n",
            "0 0\n",
            "---DATA_2----\n",
            "400 400\n",
            "0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBOFn9PEkifk"
      },
      "source": [
        "## check after cleaning status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QSO2RhTcPL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e22b79-0d52-4fe3-e4b7-8221c363690a"
      },
      "source": [
        "# check on the cleaning \n",
        "DATA_SIZE, data_dict, wrong_files = get_images_annotation_in_folders(DATA)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DATASET/DATA_0 contains 208 files: 104 images and 104 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_1 contains 800 files: 400 images and 400 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_2 contains 800 files: 400 images and 400 annotations ,else 0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1kxWzllf6PG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb43263e-7e70-4848-b051-1587a7585def"
      },
      "source": [
        "# test!\n",
        "images, annotations = data_dict[\"DATA_2\"][0],data_dict[\"DATA_2\"][1]\n",
        "images, annotations = sorted(images), sorted(annotations)\n",
        "print (images[2], annotations[2])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-05T130842.160-low-zoom25.jpg 2020-10-05T130842.160-low-zoom25.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKwV2v84ckt"
      },
      "source": [
        "we do NOT take the .jpeg because:\n",
        "- error in training of the model\n",
        "- the datasets are nicely balanced this way!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQZeL15M4yT-"
      },
      "source": [
        "# create folder \"test\" in mydir.\n",
        "all the models will be tested on the same images, 9 from each dataset\n",
        "test are handpicked to be rapresentative of the results\n",
        "only once, not needed anylonger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWJVMi7uhjjw"
      },
      "source": [
        "def collect_from_directory(dir):\n",
        "  \"\"\"\n",
        "  images, annotations = collect_from_directory(dir)\n",
        "  prints number of jpg images in a directory and number of xml files in a directory\n",
        "  returns images and annotations\n",
        "  \"\"\"\n",
        "  tot_files = os.listdir(dir)\n",
        "  annotations = [file for file in tot_files if file.endswith(\".xml\")]\n",
        "  images = [file for file in tot_files if file.endswith(\".jpg\")]\n",
        "  print (\"in %s there are annotations: %s, images: %s\" %(dir, len(images), len(annotations)))\n",
        "  return images, annotations"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jccfnBKL5iGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d200ac-3f2d-493e-afed-4c5eeaea2c15"
      },
      "source": [
        "test_folder = mydir + \"/test/\"\n",
        "try: \n",
        "    # create directory if it doesnt exist\n",
        "    os.mkdir(test_folder) \n",
        "    print (\"-created sub_dir: %s\" %test_folder)\n",
        "    test_subdir = []\n",
        "    for d in DEFORMATION[DEF:DEF+1]:\n",
        "      test_sub = test_folder+d\n",
        "      test_subdir.append(test_sub)\n",
        "      # create sub directory if it doesnt exist\n",
        "      try: \n",
        "        os.mkdir(test_sub) \n",
        "        print (\"created sub_dir: %s\" %test_sub)\n",
        "\n",
        "      except OSError as error: \n",
        "          print(error)  \n",
        "except OSError as error: \n",
        "  print(error)\n",
        "  # check number of files in folders\n",
        "  for dir in os.listdir('/content/drive/My Drive/DATASET/test/'):\n",
        "    images, annotations = collect_from_directory(test_folder +dir)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: '/content/drive/My Drive/DATASET/test/'\n",
            "in /content/drive/My Drive/DATASET/test/Low there are annotations: 9, images: 9\n",
            "in /content/drive/My Drive/DATASET/test/None there are annotations: 9, images: 9\n",
            "in /content/drive/My Drive/DATASET/test/High there are annotations: 9, images: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfHjA2zgmoXw"
      },
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnqPZ8LMr3fx",
        "outputId": "f95ce050-2b42-4c0e-e3b6-81abdbd6a3a5"
      },
      "source": [
        "# check on the cleaning \n",
        "DATA_SIZE, data_dict, wrong_files = get_images_annotation_in_folders(DATA)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DATASET/DATA_0 contains 208 files: 104 images and 104 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_1 contains 800 files: 400 images and 400 annotations ,else 0 \n",
            "/content/drive/My Drive/DATASET/DATA_2 contains 800 files: 400 images and 400 annotations ,else 0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xdObXHZnOLi"
      },
      "source": [
        "## sampling function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPz-G-oeliK"
      },
      "source": [
        "def give_sampled_files(k,sampling,mydir):\n",
        "  \"\"\"\n",
        "  move_files, move_files_check,remaining_files = give_sampled_files(k,sampling)\n",
        "  k is the dictionary key (eg DATA0)\n",
        "  sampling are the indexes choosen to be sampled\n",
        "  \"\"\"\n",
        "  # to train \n",
        "  curr_dir = mydir + '/' +k\n",
        "  all_files = (os.listdir(curr_dir))\n",
        "\n",
        "  # check for unmatch images-annotations\n",
        "  \n",
        "  #DATA_SIZE_k, data_dict_k,wrong_files_k = get_images_annotation_in_folders(str(data_dict[k]))\n",
        "\n",
        "  # clean up remaining files\n",
        "  \n",
        "\n",
        "\n",
        "  # to train \n",
        "  all_images, all_annotations = data_dict[k][0],data_dict[k][1]\n",
        "  all_images, all_annotations = sorted(all_images), sorted(all_annotations) #sorting is important!\n",
        "  move_files = []\n",
        "  move_files_check = []\n",
        "  for i in sampling:\n",
        "    this_im = all_images[i]\n",
        "    this_ann = all_annotations[i]\n",
        "    if this_im[:-4] == this_ann[:-4]: \n",
        "      move_files.append( curr_dir +'/' + this_im)\n",
        "      move_files.append ( curr_dir +'/' + this_ann)\n",
        "      move_files_check.append(this_im)\n",
        "      move_files_check.append(this_ann)\n",
        "    else:\n",
        "      print (\"error in the order of files!\")\n",
        "      \n",
        "  remaining_files = [ (curr_dir +'/' + file) for file in all_files if not file in move_files_check]\n",
        "  \n",
        "  print (\"sampled %s imgs for train\" %(len(move_files)/2))\n",
        "  print (\"remaining %s imgs are for validation \" %(len(remaining_files)/2))\n",
        "  return move_files,move_files_check,remaining_files"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEJA8NbRKlYw"
      },
      "source": [
        "## dataset parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B14lC7-AMYv"
      },
      "source": [
        "# set % of images in dataset to go for training\n",
        "P = []\n",
        "#P = [90,90,0]# percentage of images\n",
        "\n",
        "# number of images from each dataset\n",
        "if len(P)>0:\n",
        "  N = [p*DATA_SIZE[i]/200 for i,p in enumerate(P)]\n",
        "#or set directly the number of files\n",
        "N = [90,370,0]\n",
        "\n",
        "if (len(N) or len(P)) != len (DATA):\n",
        "  print (\" probabilities and number must be as many as the datasets considered\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjLNuEa9mlg5"
      },
      "source": [
        "## dataset name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v75I8nM7hAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aad0b9a-147a-469d-a1cf-751851e18228"
      },
      "source": [
        "# name the dataset\n",
        "degree360,notdeformed, deformed = int(N[0]),int(N[1]),int(N[2])\n",
        "sum_N = int(sum (N))\n",
        "dataset_name = \"tot_%s-panorama_%s-notdeformed_%s-deformed_%s\" %(sum_N,\n",
        "                                                                     degree360,\n",
        "                                                                     notdeformed, \n",
        "                                                                     deformed\n",
        "                                                                     )\n",
        "print (dataset_name)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tot_460-panorama_90-notdeformed_370-deformed_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er4eoaKJh7L1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1aeaa8d-0412-4da2-ce8b-2a65f6cdcaac"
      },
      "source": [
        "mydir"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DATASET'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MlogI0Kmg7T"
      },
      "source": [
        "## create subfolders in dataset name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4_v8Vhykje_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa878f52-34e7-436f-c4f6-0c5f84fede4a"
      },
      "source": [
        "# create new dataset folder\n",
        "main_new = mydir +\"/\"+dataset_name\n",
        "try: \n",
        "  os.mkdir(main_new)\n",
        "  print (\"created dir: %s\" %main_new)\n",
        "except OSError as error: \n",
        "    print(error)  \n",
        "    \n",
        "# define subfolders\n",
        "subfolders_name = [\"/train/\",\"/validation/\",\"/myresults/\"]\n",
        "subsubfolders_name = [\"annotations/\", \"images/\"]\n",
        "subfolders = []\n",
        "for sub in subfolders_name:\n",
        "  print (\"\")\n",
        "  print (sub)\n",
        "  path = main_new + sub\n",
        "  subfolders.append(path)\n",
        "  try: \n",
        "    os.mkdir(path) \n",
        "    print (\"-created sub_dir: %s\" %path)\n",
        "\n",
        "  except OSError as error: \n",
        "      print(error) \n",
        "\n",
        "  if sub in subfolders_name[:2]:\n",
        "    for subsub in subsubfolders_name:\n",
        "      path2 = path + subsub \n",
        "      try: \n",
        "        os.mkdir(path2) \n",
        "        print (\"--created sub_dir: %s\" %path2)\n",
        "      except OSError as error: \n",
        "        print(error) \n",
        "\n",
        "train_folder,validation_folder =  subfolders[0],subfolders[1]\n",
        "\n",
        "print (\"\")\n",
        "print (\"check\")\n",
        "print (train_folder + subsubfolders_name[0])\n",
        "print (validation_folder)\n",
        "print (dataset_name)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0\n",
            "\n",
            "/train/\n",
            "-created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/\n",
            "--created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/annotations/\n",
            "--created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/images/\n",
            "\n",
            "/validation/\n",
            "-created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/\n",
            "--created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/annotations/\n",
            "--created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/images/\n",
            "\n",
            "/myresults/\n",
            "-created sub_dir: /content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/myresults/\n",
            "\n",
            "check\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/annotations/\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/\n",
            "tot_460-panorama_90-notdeformed_370-deformed_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTT7mDfrQSuq"
      },
      "source": [
        "### correcting directory files\n",
        "they dont belong to test!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3UDn8bAluTg"
      },
      "source": [
        "check_statement = False\n",
        "remove_statement = False"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbaG8wVROQgD"
      },
      "source": [
        "if check_statement == True:\n",
        "  correcting_folder = \"/content/drive/My Drive/DATASET/test/None/\"\n",
        "  all_added_files = (os.listdir(correcting_folder))\n",
        "  content = os.listdir(DATA[0])\n",
        "  content.extend(os.listdir(DATA[1]))\n",
        "  content.extend(os.listdir(DATA[2]))\n",
        "  all_right_files =  [file for file in all_added_files if not file in content]\n",
        "  all_right_files\n",
        "  print (\"there are %s all right files\" ,len(all_right_files))\n",
        "  fuck_files =  [file for file in all_added_files if file in content]\n",
        "  print (\"there are %s wrong files\" ,len(fuck_files))\n",
        "  if remove_statement == True:\n",
        "    for file in fuck_files:\n",
        "      fullpath = correcting_folder+file\n",
        "      os.remove(fullpath)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1TNVIzPQ22l"
      },
      "source": [
        "## define images and annotations folders path for train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg4vVE_RSoHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae54bb1-16ce-4b51-943d-2e8634837256"
      },
      "source": [
        "im_train_folder = train_folder + subsubfolders_name[1]\n",
        "ann_train_folder = train_folder + subsubfolders_name[0]\n",
        "im_val_folder = validation_folder + subsubfolders_name[1]\n",
        "ann_val_folder = validation_folder + subsubfolders_name[0]\n",
        "\n",
        "print (im_train_folder)\n",
        "print (ann_train_folder)\n",
        "print (im_val_folder)\n",
        "print (ann_val_folder)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/images/\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/train/annotations/\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/images/\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0/validation/annotations/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbaeYPeiwj"
      },
      "source": [
        "## sample files for training, the rest is for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHe1Ea_0diR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2459c13f-0450-41d7-80e6-93111cfea6d4"
      },
      "source": [
        "files_in_validation_already = 0\n",
        "for i in range(len(DATA)):\n",
        "  k = DATA[i]\n",
        "  \n",
        "  tot_size = DATA_SIZE[i]/2\n",
        "  train_N = int(N[i])\n",
        "\n",
        "  print (\"\")\n",
        "  print (\" %s there are in total %s \" %(k, tot_size))\n",
        " \n",
        "  list_indexes = list(range(train_N))\n",
        "  sampling = random.sample(list_indexes, k=train_N)\n",
        "\n",
        "  move_files, move_files_check,remaining_files = give_sampled_files(k,sampling,mydir)\n",
        "\n",
        "  if tot_size != len(move_files)/2 +len(remaining_files)/2:\n",
        "    print (\"error! lost some files? \")\n",
        "  else:\n",
        "\n",
        "\n",
        "    #sampled files go to train set\n",
        "    print (\"copying files in train\")\n",
        "    for file in move_files:\n",
        "      extension = file.split('.')[-1]\n",
        "      if extension == \"jpg\":\n",
        "        shutil.copy2(file, im_train_folder) # target filename is /dst/dir/file.ext\n",
        "      elif extension == \"xml\":\n",
        "        shutil.copy2(file,ann_train_folder)\n",
        "    \n",
        "    #remaining file go to validation set\n",
        "    print (\"copying files in eval\")\n",
        "\n",
        "    # count how many files are already in validation\n",
        "    #files_in_validation_already = os.listdir(im_val_folder)\n",
        "\n",
        "    # take only as many as to reach 200 for validation\n",
        "    max_validation = 200\n",
        "    difference = max_validation- (files_in_validation_already)\n",
        "    print (\"there are already %s files for evaluation\" %((files_in_validation_already)))\n",
        "\n",
        "    limit = min(int(len(remaining_files)/2),difference)\n",
        "\n",
        "    #print (\"\")\n",
        "    print (\"chosing from %s remaining files\" %len(remaining_files/2))\n",
        "    #for i in remaining_files:\n",
        "      #print (i)\n",
        "    count_ann = 0\n",
        "    count_im = 0\n",
        "    #print (\"\")\n",
        "\n",
        "\n",
        "    for file in (remaining_files)[:limit]:\n",
        "      extension = file.split('.')[-1]\n",
        "      name = file[:-4]\n",
        "\n",
        "      file_im = name +\".jpg\"\n",
        "      file_ann = name + \".xml\"\n",
        "      #if extension == \"jpg\":\n",
        "      shutil.copy2(file_im, im_val_folder) # target filename is /dst/dir/file.ext\n",
        "      count_im += 1\n",
        "      #elif extension == \"xml\":\n",
        "      shutil.copy2(file_ann,ann_val_folder)\n",
        "      count_ann += 1\n",
        "    files_in_validation_already += limit\n",
        "    print (\"added further %s img and %s ann for validation\" %(count_im,count_ann))\n",
        "\n",
        "print (\"\")\n",
        "print (mydir +'/'+ dataset_name)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " DATA_0 there are in total 104.0 \n",
            "sampled 90.0 imgs for train\n",
            "remaining 14.0 imgs are for validation \n",
            "copying files in train\n",
            "copying files in eval\n",
            "there are already 0 files for evaluation\n",
            "chosing from 28 remaining files\n",
            "added further 14 img and 14 ann for validation\n",
            "\n",
            " DATA_1 there are in total 400.0 \n",
            "sampled 370.0 imgs for train\n",
            "remaining 30.0 imgs are for validation \n",
            "copying files in train\n",
            "copying files in eval\n",
            "there are already 14 files for evaluation\n",
            "chosing from 60 remaining files\n",
            "added further 30 img and 30 ann for validation\n",
            "\n",
            " DATA_2 there are in total 400.0 \n",
            "sampled 0.0 imgs for train\n",
            "remaining 400.0 imgs are for validation \n",
            "copying files in train\n",
            "copying files in eval\n",
            "there are already 44 files for evaluation\n",
            "chosing from 800 remaining files\n",
            "added further 156 img and 156 ann for validation\n",
            "\n",
            "/content/drive/My Drive/DATASET/tot_460-panorama_90-notdeformed_370-deformed_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kAUdqJflX19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d373726-be58-46e8-fb9e-f12f801eab31"
      },
      "source": [
        "mydir"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/DATASET'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT6pILq5E6AA"
      },
      "source": [
        "# zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dhvfsfGdQ9w"
      },
      "source": [
        "if in the shared drive, move them manually to my own!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6VZsNZ7KC97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2547bf94-a967-4e82-ea15-ed4adb7bbac2"
      },
      "source": [
        "#!ls \"/content/drive/My Drive/data/\" # in my own drive\n",
        "!ls \"/content/drive/My Drive/DATASET/\" # in shared drive\n",
        "zipping_directory = \"fire extinguisher\"\n",
        "#storing_folder = \"/content/drive/My Drive/data/fire extinguisher/\"\n",
        "storing_folder = \"/content/drive/My Drive/DATASET/fire extinguisher/\""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA_0\tdatasets_overview.xlsx\ttot_460-panorama_90-notdeformed_370-deformed_0\n",
            "DATA_1\tdata.zip\n",
            "DATA_2\ttest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VqTu5mlKHqL"
      },
      "source": [
        "import shutil\n",
        "zipping_directory = \"test\"\n",
        "#storing_folder = mydir +'/'+ dataset_name\n",
        "storing_folder = mydir +'/'+ zipping_directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGlqKQli21Ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "c4ce6e47-bb5e-4114-b657-bffdb7452969"
      },
      "source": [
        "shutil.make_archive(\n",
        "  zipping_directory, \n",
        "  'zip',           # the archive format - or tar, bztar, gztar \n",
        "  root_dir=storing_folder,   # root for archive - current working dir if None\n",
        "  base_dir=storing_folder)   # start archiving from here - cwd if None too"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-d8d2beec039a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# the archive format - or tar, bztar, gztar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mroot_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstoring_folder\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# root for archive - current working dir if None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   base_dir=storing_folder)   # start archiving from here - cwd if None too\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger)\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                         \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adding '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type)\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 95] Operation not supported: '/content/drive/My Drive/data/fire extinguisher deformed/validation/use validation from other folder.gdoc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1XSUGHOLwk"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPuQUl7N2A1s"
      },
      "source": [
        "# define subfolders\n",
        "#subfolders = [\"/test/\", \"/train/\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ge-xeqi5RX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb20dc0-a259-4c99-e7c0-4518f3675679"
      },
      "source": [
        "# check naming \n",
        "for i in range(len(subfolders)):\n",
        "  print (mydir + subfolders[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/DATASET/test/\n",
            "/content/drive/My Drive/DATASET/train/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOj8vuA6682C"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl-DN4Ja6q_w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}