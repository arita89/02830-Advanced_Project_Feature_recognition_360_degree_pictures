# -*- coding: utf-8 -*-
"""datasets_creation_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QLi1mnwSDZfssh2BHKGzzvbKf39z4uJs
"""

# function that takes 1 to 4 inputs and creates a different balanced datasets
# DATA0.0 360 degree images from Google dataset
# DATA0.1 360 degree images of our own

# DATA0 360 degree images with fire extinguishers (this is what was in the validation folder)
# DATA1 normal images from internet (oldouz labels )
# DATA2 deformed images with data augmentation (ari labels)

"""# import"""

import os
# importing shutil module  
import shutil
import random

"""## google drive mount"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# directories"""

#change work directory
os.chdir("/content/drive/My Drive/DATASET/")

# change work directory and check on where you are 
!ls

# var my directory
mydir = os.getcwd()
mydir

"""## data"""

# store subfolders
DATASET = os.listdir(mydir)
DATA = sorted(DATASET)[:3]
DATA

DATASET = os.listdir(mydir)
DATASET

"""## deformation"""

DEFORMATION = ["High","Low", "None"]
DEF = 1
DEFORMATION[DEF:DEF+1]

"""## check file type

.png not admitted
"""

#taking the extension out

def find_all_extension_infolder(destination):
  SplitTypes=[]
  for file in os.listdir(destination):
      SplitTypes.append(file.split('.')[-1])
  set_extensions = list(set(SplitTypes))
  return (set_extensions)

# checking extensions out 
DATA_SIZE = []
for sub in DATA:
  set_extensions = find_all_extension_infolder(sub)
  print (set_extensions)

#delete_extensions = ['jpeg', 'png']

"""## check folders

take as images input only ,jpg files
controls that for each images there is one annotation and vice versa
discard the rest
"""

# count the number of elements in each subfolder
# rememeber images and annotations are together so the number should be even 
def get_images_annotation_in_folders(DATA):
  """
  DATA is a vector of sub folders
  DATA_SIZE, data_dict,wrong_files = get_images_annotation_in_folders(DATA)
  """
  DATA_SIZE = []
  data_dict =  dict()
  wrong_files = list()
  for sub in DATA:
    sub_dir =  (mydir + '/' +sub)
    all_files = os.listdir(sub_dir)
    DATA_SIZE.append(len(all_files))
    annotations = [file for file in all_files if file.endswith(".xml")]
    #images = [file for file in all_files if (file.endswith(".jpg") or file.endswith(".jpeg") or file.endswith(".png")) ] 
    images = [file for file in all_files if (file.endswith(".jpg"))]
    others = []
    #others = [file for file in others if not file in annotations ]
    others = [file for file in all_files if (file.endswith(".jpeg") or file.endswith(".png"))]
    wrong_files.extend(others)
    data_dict[sub] = (images, annotations)
    check = ""
    if len(annotations) != len(images):
      check = "<------------ shite"
    print ("%s contains %s files: %s images and %s annotations ,else %s %s" %(sub_dir , len(all_files), len(images), len(annotations),len(others),check))
    #if len(others) > 0:
      #print (others)
  return DATA_SIZE, data_dict, wrong_files

DATA_SIZE, data_dict,wrong_files = get_images_annotation_in_folders(DATA)

DATA_SIZE

# remove unmatching images / annotations
# refresh the data_dict!
images_to_delete = []
annotations_to_delete = []
for k in data_dict.keys():
  print ("---%s----" %k)
  images, annotations = data_dict[k] # refresh the data_dict!
  print (len(images), len(annotations))
  set_a = set([ x[:-4] for x in images])
  set_b = set([ x[:-4] for x in annotations])
  images_to_delete = set_a-set_b
  annotations_to_delete = set_b-set_a
  print (len(images_to_delete),len(annotations_to_delete))

  for im in images_to_delete:
      fullpath = mydir+'/'+ k +'/'+ im + ".jpg"
      print (fullpath)
      os.remove(fullpath)

  for annotation in annotations_to_delete:
      fullpath = mydir+'/'+ k +'/'+ annotation + ".xml"
      print (fullpath)
      os.remove(fullpath)

"""## check after cleaning status"""

# check on the cleaning 
DATA_SIZE, data_dict, wrong_files = get_images_annotation_in_folders(DATA)

# test!
images, annotations = data_dict["DATA_2"][0],data_dict["DATA_2"][1]
images, annotations = sorted(images), sorted(annotations)
print (images[2], annotations[2])

"""we do NOT take the .jpeg because:
- error in training of the model
- the datasets are nicely balanced this way!

# create folder "test" in mydir.
all the models will be tested on the same images, 9 from each dataset
test are handpicked to be rapresentative of the results
only once, not needed anylonger
"""

def collect_from_directory(dir):
  """
  images, annotations = collect_from_directory(dir)
  prints number of jpg images in a directory and number of xml files in a directory
  returns images and annotations
  """
  tot_files = os.listdir(dir)
  annotations = [file for file in tot_files if file.endswith(".xml")]
  images = [file for file in tot_files if file.endswith(".jpg")]
  print ("in %s there are annotations: %s, images: %s" %(dir, len(images), len(annotations)))
  return images, annotations

test_folder = mydir + "/test/"
try: 
    # create directory if it doesnt exist
    os.mkdir(test_folder) 
    print ("-created sub_dir: %s" %test_folder)
    test_subdir = []
    for d in DEFORMATION[DEF:DEF+1]:
      test_sub = test_folder+d
      test_subdir.append(test_sub)
      # create sub directory if it doesnt exist
      try: 
        os.mkdir(test_sub) 
        print ("created sub_dir: %s" %test_sub)

      except OSError as error: 
          print(error)  
except OSError as error: 
  print(error)
  # check number of files in folders
  for dir in os.listdir('/content/drive/My Drive/DATASET/test/'):
    images, annotations = collect_from_directory(test_folder +dir)

"""# MAIN"""

# check on the cleaning 
DATA_SIZE, data_dict, wrong_files = get_images_annotation_in_folders(DATA)

"""## sampling function"""

def give_sampled_files(k,sampling,mydir):
  """
  move_files, move_files_check,remaining_files = give_sampled_files(k,sampling)
  k is the dictionary key (eg DATA0)
  sampling are the indexes choosen to be sampled
  """
  # to train 
  curr_dir = mydir + '/' +k
  all_files = (os.listdir(curr_dir))

  # check for unmatch images-annotations
  
  #DATA_SIZE_k, data_dict_k,wrong_files_k = get_images_annotation_in_folders(str(data_dict[k]))

  # clean up remaining files
  


  # to train 
  all_images, all_annotations = data_dict[k][0],data_dict[k][1]
  all_images, all_annotations = sorted(all_images), sorted(all_annotations) #sorting is important!
  move_files = []
  move_files_check = []
  for i in sampling:
    this_im = all_images[i]
    this_ann = all_annotations[i]
    if this_im[:-4] == this_ann[:-4]: 
      move_files.append( curr_dir +'/' + this_im)
      move_files.append ( curr_dir +'/' + this_ann)
      move_files_check.append(this_im)
      move_files_check.append(this_ann)
    else:
      print ("error in the order of files!")
      
  remaining_files = [ (curr_dir +'/' + file) for file in all_files if not file in move_files_check]
  
  print ("sampled %s imgs for train" %(len(move_files)/2))
  print ("remaining %s imgs are for validation " %(len(remaining_files)/2))
  return move_files,move_files_check,remaining_files

"""## dataset parameters



"""

# set % of images in dataset to go for training
P = []
#P = [14,43,43]# percentage of images

# number of images from each dataset
if len(P)>0:
  N = [p*DATA_SIZE[i]/200 for i,p in enumerate(P)]
#or set directly the number of files
N = [90,300,300]

if (len(N) or len(P)) != len (DATA):
  print (" probabilities and number must be as many as the datasets considered")

"""## dataset name"""

# name the dataset
degree360,notdeformed, deformed = int(N[0]),int(N[1]),int(N[2])
sum_N = int(sum (N))
dataset_name = "tot_%s-panorama_%s-notdeformed_%s-deformed_%s" %(sum_N,
                                                                     degree360,
                                                                     notdeformed, 
                                                                     deformed
                                                                     )
print (dataset_name)

mydir

"""## create subfolders in dataset name"""

# create new dataset folder
main_new = mydir +"/"+dataset_name
try: 
  os.mkdir(main_new)
  print ("created dir: %s" %main_new)
except OSError as error: 
    print(error)  
    
# define subfolders
subfolders_name = ["/train/","/validation/","/myresults/"]
subsubfolders_name = ["annotations/", "images/"]
subfolders = []
for sub in subfolders_name:
  print ("")
  print (sub)
  path = main_new + sub
  subfolders.append(path)
  try: 
    os.mkdir(path) 
    print ("-created sub_dir: %s" %path)

  except OSError as error: 
      print(error) 

  if sub in subfolders_name[:2]:
    for subsub in subsubfolders_name:
      path2 = path + subsub 
      try: 
        os.mkdir(path2) 
        print ("--created sub_dir: %s" %path2)
      except OSError as error: 
        print(error) 

train_folder,validation_folder =  subfolders[0],subfolders[1]

print ("")
print ("check")
print (train_folder + subsubfolders_name[0])
print (validation_folder)
print (dataset_name)

"""### correcting directory files
they dont belong to test!
"""

check_statement = True
remove_statement = False

if check_statement == True:
  correcting_folder = "/content/drive/My Drive/DATASET/test/None/"
  all_added_files = (os.listdir(correcting_folder))
  content = os.listdir(DATA[0])
  content.extend(os.listdir(DATA[1]))
  content.extend(os.listdir(DATA[2]))
  all_right_files =  [file for file in all_added_files if not file in content]
  all_right_files
  print ("there are %s all right files" %len(all_right_files))
  fuck_files =  [file for file in all_added_files if file in content]
  print ("there are %s wrong files" %len(fuck_files))
  if remove_statement == True:
    for file in fuck_files:
      fullpath = correcting_folder+file
      os.remove(fullpath)

"""## define images and annotations folders path for train and validation"""

im_train_folder = train_folder + subsubfolders_name[1]
ann_train_folder = train_folder + subsubfolders_name[0]
im_val_folder = validation_folder + subsubfolders_name[1]
ann_val_folder = validation_folder + subsubfolders_name[0]

print (im_train_folder)
print (ann_train_folder)
print (im_val_folder)
print (ann_val_folder)

"""## sample files for training, the rest is for validation"""

# check on the cleaning 
DATA_SIZE, data_dict, wrong_files = get_images_annotation_in_folders(DATA)

#balance the validation set
im_panoramic = 104-90
im_non_def = 86
im_def  = 0
all_max_validation = [im_panoramic,im_non_def,im_def]
sum_val = sum(all_max_validation)
degree360,notdeformed, deformed = int(N[0]),int(N[1]),int(N[2])

validation_set = "tot_%s-panorama_%s-notdeformed_%s-deformed_%s" %(sum_val,
                                                                     im_panoramic,
                                                                     im_non_def, 
                                                                     im_def
                                                                     )
print (validation_set)

print (dataset_name)

files_in_validation_already = 0
for i in range(len(DATA)):
  k = DATA[i]
  
  tot_size = DATA_SIZE[i]
  train_N = int(N[i])

  print ("")
  print (" %s has a total of %s files - images and annotations)" %(k, tot_size))
 
  list_indexes = list(range(train_N))
  sampling = random.sample(list_indexes, k=train_N)

  move_files, move_files_check,remaining_files = give_sampled_files(k,sampling,mydir)

  train_images = [file for file in move_files if file[-3:] == 'jpg' ]
  train_ann = [file for file in move_files if file[-3:] == 'xml' ]

  remaining_images = [file for file in remaining_files if file[-3:] == 'jpg' ]
  remaining_ann = [file for file in remaining_files if file[-3:] == 'xml' ]

  print ("sampled %s img for training and the remaining %s img for validation" %(len(train_images),len(remaining_images)))
  print ("sampled %s ann for training and the remaining %s ann for validation" %(len(train_ann),len(remaining_ann)))

  if tot_size != len(move_files) +len(remaining_files):
  #if tot_size != len(remaining_images) +len(remaining_ann):
    print ("error! lost some files? ")
  else:


    #sampled files go to train set
    print ("copying files in train")
    for file in move_files:
      extension = file.split('.')[-1]
      if extension == "jpg":
        shutil.copy2(file, im_train_folder) # target filename is /dst/dir/file.ext
      elif extension == "xml":
        shutil.copy2(file,ann_train_folder)
    
    #remaining file go to validation set
    print ("copying files in eval")

    # count how many files are already in validation
    img_in_validation_already = os.listdir(im_val_folder)
    ann_in_validation_already = os.listdir(ann_val_folder)

    # take only as many as to reach 200 for validation
    max_validation = all_max_validation[i]

    if max_validation != 0:
      #difference = max_validation- (files_in_validation_already)
      print ("there are already %s images adn %s ann for evaluation" %((img_in_validation_already,ann_in_validation_already)))
      limit = min(len(remaining_images),max_validation)
    
    else:
      limit = 0

    #print ("")
    print ("chosing at most %s from %s remaining img" %(limit,len(remaining_images)))
    print ("chosing at most %s from %s remaining ann" %(limit,len(remaining_ann)))
    #for i in remaining_files:
      #print (i)
    count_ann = 0
    count_im = 0
    tot_count = count_ann+count_im
    #print ("")

    if max_validation != 0:
      while tot_count <= limit: 
        for file in remaining_files[:limit]:
          extension = file[-3:]
          name = file[:-4]

          file_im = name +".jpg"
          file_ann = name + ".xml"
          #if extension == "jpg":
          shutil.copy2(file_im, im_val_folder) # target filename is /dst/dir/file.ext
          count_im += 0.5
          #if extension == "xml":
          shutil.copy2(file_ann,ann_val_folder)
          count_ann += 0.5
          tot_count = count_im + count_ann
    else: 
      print ("no files to add")
    files_in_validation_already += limit
    print ("added further %s img and %s ann for validation" %(count_im,count_ann))

print ("")
print (mydir +'/'+ dataset_name)

testname = "blablabla.jpg"
name = testname[:-4]
ext = testname[-3:]
ext

# double checkÂ¨
post_dir = "/content/drive/My Drive/DATASET/tot_690-panorama_90-notdeformed_300-deformed_300"
post_sub = os.listdir("/content/drive/My Drive/DATASET/tot_690-panorama_90-notdeformed_300-deformed_300")
for sub in post_sub:
  this_sub = post_dir+'/' +sub
  subsub = os.listdir(this_sub)
  all_files_sub = []
  for mysubsub in subsub:
    this_subsub = this_sub +'/'+ mysubsub
    all_files = os.listdir(this_subsub)
    all_files_sub += all_files
    print (this_subsub, len(all_files_sub))
  
  annotations = [file for file in all_files_sub if file.endswith(".xml")]
  images = [file for file in all_files_sub if (file.endswith(".jpg"))]
  others = [file for file in all_files_sub if (file.endswith(".jpeg") or file.endswith(".png"))]
  print (len(images), len(annotations))
  wrong_annotations = [file for file in annotations if not (file[:-4]+".jpg") in images ]
  print (wrong_annotations)

"""# zip files

if in the shared drive, move them manually to my own!
"""

#!ls "/content/drive/My Drive/data/" # in my own drive
!ls "/content/drive/My Drive/DATASET/" # in shared drive
zipping_directory = "fire extinguisher"
#storing_folder = "/content/drive/My Drive/data/fire extinguisher/"
storing_folder = "/content/drive/My Drive/DATASET/fire extinguisher/"

import shutil
zipping_directory = "test"
#storing_folder = mydir +'/'+ dataset_name
storing_folder = mydir +'/'+ zipping_directory

shutil.make_archive(
  zipping_directory, 
  'zip',           # the archive format - or tar, bztar, gztar 
  root_dir=storing_folder,   # root for archive - current working dir if None
  base_dir=storing_folder)   # start archiving from here - cwd if None too